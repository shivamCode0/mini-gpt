{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-09-28 21:07:02--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'tmp/data.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  4%  248K 4s\n",
      "    50K .......... .......... .......... .......... ..........  9%  464K 3s\n",
      "   100K .......... .......... .......... .......... .......... 13%  605K 2s\n",
      "   150K .......... .......... .......... .......... .......... 18%  689K 2s\n",
      "   200K .......... .......... .......... .......... .......... 22%  610K 2s\n",
      "   250K .......... .......... .......... .......... .......... 27%  780K 2s\n",
      "   300K .......... .......... .......... .......... .......... 32%  574K 1s\n",
      "   350K .......... .......... .......... .......... .......... 36%  464K 1s\n",
      "   400K .......... .......... .......... .......... .......... 41%  517K 1s\n",
      "   450K .......... .......... .......... .......... .......... 45%  159K 1s\n",
      "   500K .......... .......... .......... .......... .......... 50%  932K 1s\n",
      "   550K .......... .......... .......... .......... .......... 55% 1.91M 1s\n",
      "   600K .......... .......... .......... .......... .......... 59%  308K 1s\n",
      "   650K .......... .......... .......... .......... .......... 64%  698K 1s\n",
      "   700K .......... .......... .......... .......... .......... 68%  618K 1s\n",
      "   750K .......... .......... .......... .......... .......... 73%  810K 1s\n",
      "   800K .......... .......... .......... .......... .......... 78%  743K 0s\n",
      "   850K .......... .......... .......... .......... .......... 82%  556K 0s\n",
      "   900K .......... .......... .......... .......... .......... 87%  341K 0s\n",
      "   950K .......... .......... .......... .......... .......... 91%  446K 0s\n",
      "  1000K .......... .......... .......... .......... .......... 96%  480K 0s\n",
      "  1050K .......... .......... .......... .........            100%  651K=2.3s\n",
      "\n",
      "2023-09-28 21:07:05 (484 KB/s) - 'tmp/data.txt' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download file to tmp/data.txt\n",
    "!wget -O tmp/data.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('tmp/data.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print('text length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "chars = [\"[PAD]\", *sorted(list(set(text)))]\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 48, 48, 2, 59, 47, 44, 57, 44]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from character to index and vice versa\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([19, 48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44, 45,\n",
      "        54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40, 53, 64,\n",
      "         2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57,  2, 52, 44,  2,\n",
      "        58, 55, 44, 40, 50,  9,  1,  1, 14, 51, 51, 11,  1, 32, 55, 44, 40, 50,\n",
      "         7,  2, 58, 55, 44, 40, 50,  9,  1,  1, 19, 48, 57, 58, 59,  2, 16, 48,\n",
      "        59, 48, 65, 44, 53, 11,  1, 38, 54, 60], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# store in tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.int64, device=device)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # the 100 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44, 45,\n",
       "        54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40, 53, 64,\n",
       "         2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57,  2, 52, 44,  2,\n",
       "        58, 55, 44, 40, 50,  9,  1,  1, 14, 51, 51, 11,  1, 32, 55, 44, 40, 50,\n",
       "         7,  2, 58, 55, 44, 40, 50,  9,  1,  1, 19, 48, 57, 58, 59,  2, 16, 48,\n",
       "        59, 48, 65, 44, 53, 11,  1, 38, 54, 60,  2], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 100\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 10]) torch.int64 cuda:0\n",
      "targets:\n",
      "torch.Size([32, 10]) torch.int64 cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 10 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape, xb.dtype, xb.device)\n",
    "print('targets:')\n",
    "print(yb.shape, yb.dtype, yb.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n):\n",
    "        super().__init__()\n",
    "        super().to(device)\n",
    "        self.n = n\n",
    "        assert n >= 3, \"n should be at least 3\"\n",
    "        embedding_size = int(vocab_size / 1.5 + 10)\n",
    "        intermediate_size = vocab_size + 10 * n\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.fc = nn.Linear(embedding_size * n, intermediate_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.final = nn.Linear(intermediate_size, vocab_size)\n",
    "\n",
    "    # Create separate function for forward calculation\n",
    "    def forward(self, x, only_last=False):\n",
    "        assert len(x.shape) == 2, \"input shape should be (batch, time)\"\n",
    "\n",
    "        if only_last:\n",
    "            # pad time dim to at least n\n",
    "            x = F.pad(x, (self.n - 1, 0), value=0)\n",
    "            B, T = x.shape\n",
    "            x = x.view(B, 1, T)\n",
    "        else:\n",
    "            new_x = torch.zeros((x.shape[0], x.shape[1], self.n), dtype=torch.int64, device=device) - 69\n",
    "            # -69 is just a random number so if we see it we know something went wrong\n",
    "            for t in range(x.shape[1]):\n",
    "                for pos in range(self.n):\n",
    "                    row = x[:, max(0, t - pos) : t + 1]\n",
    "                    row = F.pad(row, (self.n - row.shape[1], 0), value=0)\n",
    "                    row = F.pad(row, (self.n - row.shape[1], 0), value=0)\n",
    "                    new_x[:, t] = row\n",
    "\n",
    "            x = new_x\n",
    "\n",
    "        x = self.token_embedding(x)\n",
    "        x = x.view(x.shape[0], x.shape[1], -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.final(x)\n",
    "        x\n",
    "        return x\n",
    "\n",
    "    # def forward(self, x, targets=None):\n",
    "    #     if targets is None:\n",
    "    #         # we're doing inference, so we don't have targets\n",
    "    #         # if x is None, we're generating from scratch\n",
    "    #         if x is None:\n",
    "    #             x = batch_size\n",
    "\n",
    "    #         # generate x batches\n",
    "    #         if isinstance(x, int):\n",
    "    #             x = torch.zeros(x, 1, dtype=torch.int32, device=device)\n",
    "    #         return self.forward_calc(x), None\n",
    "\n",
    "    #     else:\n",
    "    #         # we're training, so we do have targets\n",
    "    #         logits = self.forward_calc(x)\n",
    "    #         B, T, C = logits.shape  # batch, time, channel\n",
    "    #         logits_flat = logits.view(B * T, C)\n",
    "    #         loss = F.cross_entropy(logits_flat, targets.view(B * T))\n",
    "\n",
    "    #         return logits_flat, loss\n",
    "\n",
    "    def loss(self, logits, targets):\n",
    "        B, T, C = logits.shape\n",
    "        logits_flat = logits.view(B * T, C)\n",
    "        loss = F.cross_entropy(logits_flat, targets.view(B * T))\n",
    "        return loss\n",
    "\n",
    "    def generate(self, x, max_len_new, temperature=1.0):\n",
    "        for _ in range(max_len_new):\n",
    "            logits = self(x)[:, -1] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Mult-Adds\n",
      "============================================================================================================================================\n",
      "NGramLanguageModel                       [32, 10]                  [32, 10, 66]              --                        --\n",
      "├─Embedding: 1-1                         [32, 10, 7]               [32, 10, 7, 54]           3,564                     114,048\n",
      "│    └─weight                                                                                └─3,564\n",
      "├─Linear: 1-2                            [32, 10, 378]             [32, 10, 136]             51,544                    1,649,408\n",
      "│    └─weight                                                                                ├─51,408\n",
      "│    └─bias                                                                                  └─136\n",
      "├─ReLU: 1-3                              [32, 10, 136]             [32, 10, 136]             --                        --\n",
      "├─Dropout: 1-4                           [32, 10, 136]             [32, 10, 136]             --                        --\n",
      "├─Linear: 1-5                            [32, 10, 136]             [32, 10, 66]              9,042                     289,344\n",
      "│    └─weight                                                                                ├─8,976\n",
      "│    └─bias                                                                                  └─66\n",
      "============================================================================================================================================\n",
      "Total params: 64,150\n",
      "Trainable params: 64,150\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 2.05\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.48\n",
      "Params size (MB): 0.26\n",
      "Estimated Total Size (MB): 1.74\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NGramLanguageModel(\n",
       "  (token_embedding): Embedding(66, 54, padding_idx=0)\n",
       "  (fc): Linear(in_features=378, out_features=136, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (final): Linear(in_features=136, out_features=66, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NGramLanguageModel(vocab_size, 7)\n",
    "summary(model, input_size=(32, 10), dtypes=[torch.long], verbose=2, device=device, col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGramLanguageModel(\n",
      "  (token_embedding): Embedding(66, 54, padding_idx=0)\n",
      "  (fc): Linear(in_features=378, out_features=136, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (final): Linear(in_features=136, out_features=66, bias=True)\n",
      ")\n",
      "logits: torch.Size([256, 10, 66])\n",
      "loss: tensor(4.2138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "wear she b'xkfE-z?FT\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# logits, loss = model(xb, yb)\n",
    "logits = model(xb)\n",
    "loss = model.loss(logits, yb)\n",
    "print('logits:', logits.shape)\n",
    "print('loss:', loss)\n",
    "\n",
    "print(decode(model.generate(xb, 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.212\n",
      "step: 50, loss: 2.313\n",
      "step: 100, loss: 2.140\n",
      "step: 150, loss: 2.026\n",
      "step: 200, loss: 2.023\n",
      "step: 250, loss: 1.984\n",
      "step: 300, loss: 1.925\n",
      "step: 350, loss: 1.977\n",
      "step: 400, loss: 1.911\n",
      "step: 450, loss: 1.888\n",
      "step: 500, loss: 1.894\n",
      "step: 550, loss: 1.915\n",
      "step: 600, loss: 1.931\n",
      "step: 650, loss: 1.862\n",
      "step: 700, loss: 1.877\n",
      "step: 750, loss: 1.834\n",
      "step: 800, loss: 1.816\n",
      "step: 850, loss: 1.813\n",
      "step: 900, loss: 1.862\n",
      "step: 950, loss: 1.849\n",
      "step: 1000, loss: 1.794\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.004)\n",
    "batch_size = 256\n",
    "for step in range(1001):\n",
    "    xb, yb = get_batch('train')\n",
    "    # logits, loss = model(xb, yb)\n",
    "    logits = model(xb)\n",
    "    loss = model.loss(logits, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 50 == 0:\n",
    "        print(f'step: {step}, loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUCENTIO:\n",
      "Youpon their shall kenwermanous,\n",
      "Where ore nubied: and shall from thy faire,\n",
      "Is foucts of Farewill to eneHy ese. Chenole Catleryou museart then mide!\n",
      "\n",
      "KING RICHARD:\n",
      "Which swilld ad ame,\n",
      "That He ti\n",
      "----\n",
      "LUCENTIO:\n",
      "he that me me, ay to sould serve onfor and much one that opost assalt offe tian shast thy for rield; mothit bas I palsader dawnery we swell:\n",
      "Ay Marry for on her viccance, my bruck ow you are much:\n",
      "----\n",
      "LUCENTIO:\n",
      "Diven agay will my can he but when for his\n",
      "Tybalted repul hid feath; morkity! thy bast thee bral a ging hist me\n",
      "Wherefole!\n",
      "Sy that yeak an Sime.\n",
      "\n",
      "Lord I had come the farge your kings, you, world a\n",
      "----\n",
      "LUCENTIO:\n",
      "Wold nother. Ance, fece\n",
      "Which onfell.\n",
      "Thene wotion\n",
      "the cher, my life: umplencetengad-forspars not forget with daboa!\n",
      "\n",
      "Farge you with swold, in the king our gath'd hath dend to wrich it good upon m\n",
      "----\n",
      "LUCENTIO:\n",
      "Forion: prosmine to love, the well reto have evence, my pien's is facender and unbleould seeps even ither.\n",
      "\n",
      "PAULINA:\n",
      "Which her not, But women to be\n",
      "Edwirt; I shan, and that foun.\n",
      "\n",
      "CORIOLANUS:\n",
      "What\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for x in model.generate(torch.tensor([encode(\"LUCENT\")] * 5, device=device), 200, 0.9):\n",
    "    print(decode(x.tolist()))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[47, 44, 51, 51, 54, 62, 44, 57,  2, 59, 47, 44,  2, 58, 55]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(torch.tensor([encode(\"hello\")], device=device), 10, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_20404\\978116475.py:19: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if only_last:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x864 and 378x136)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shiva\\Documents\\GitHub\\mini-gpt\\ngram-V2.ipynb Cell 20\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtmp/ngram/config.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     json\u001b[39m.\u001b[39mdump({\u001b[39m\"\u001b[39m\u001b[39mchars\u001b[39m\u001b[39m\"\u001b[39m: chars, \u001b[39m\"\u001b[39m\u001b[39mvocab_size\u001b[39m\u001b[39m\"\u001b[39m: vocab_size, \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: model\u001b[39m.\u001b[39mn}, f)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     (torch\u001b[39m.\u001b[39;49mzeros(\u001b[39m1\u001b[39;49m, \u001b[39m10\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint32, device\u001b[39m=\u001b[39;49mdevice), \u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtmp/ngram/model.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     input_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     output_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49m{\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\u001b[39m0\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\u001b[39m0\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     },\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\onnx\\utils.py:504\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[0;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[0;32m    188\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    204\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    205\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \n\u001b[0;32m    208\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 504\u001b[0m     _export(\n\u001b[0;32m    505\u001b[0m         model,\n\u001b[0;32m    506\u001b[0m         args,\n\u001b[0;32m    507\u001b[0m         f,\n\u001b[0;32m    508\u001b[0m         export_params,\n\u001b[0;32m    509\u001b[0m         verbose,\n\u001b[0;32m    510\u001b[0m         training,\n\u001b[0;32m    511\u001b[0m         input_names,\n\u001b[0;32m    512\u001b[0m         output_names,\n\u001b[0;32m    513\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[0;32m    514\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[0;32m    515\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[0;32m    516\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[0;32m    517\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[0;32m    518\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[0;32m    519\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[0;32m    520\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\onnx\\utils.py:1529\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[0;32m   1526\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1527\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[1;32m-> 1529\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[0;32m   1530\u001b[0m     model,\n\u001b[0;32m   1531\u001b[0m     args,\n\u001b[0;32m   1532\u001b[0m     verbose,\n\u001b[0;32m   1533\u001b[0m     input_names,\n\u001b[0;32m   1534\u001b[0m     output_names,\n\u001b[0;32m   1535\u001b[0m     operator_export_type,\n\u001b[0;32m   1536\u001b[0m     val_do_constant_folding,\n\u001b[0;32m   1537\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[0;32m   1538\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m   1539\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[0;32m   1540\u001b[0m )\n\u001b[0;32m   1542\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[0;32m   1544\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[0;32m   1545\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\onnx\\utils.py:1111\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[1;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m   1110\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[1;32m-> 1111\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[0;32m   1112\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[0;32m   1114\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\onnx\\utils.py:987\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m    982\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[0;32m    983\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     )\n\u001b[0;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 987\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[0;32m    988\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m    989\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\onnx\\utils.py:891\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m    889\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[0;32m    890\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 891\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[0;32m    892\u001b[0m     model,\n\u001b[0;32m    893\u001b[0m     args,\n\u001b[0;32m    894\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    895\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    896\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    897\u001b[0m )\n\u001b[0;32m    898\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[0;32m    900\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\jit\\_trace.py:1184\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[1;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[0;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m   1183\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m-> 1184\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1185\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\jit\\_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[1;32m--> 127\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[0;32m    128\u001b[0m     wrapper,\n\u001b[0;32m    129\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[0;32m    130\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[0;32m    131\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[0;32m    133\u001b[0m )\n\u001b[0;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[0;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\jit\\_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    117\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[1;32m--> 118\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    120\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1182\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1180\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1183\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1184\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "\u001b[1;32mc:\\Users\\shiva\\Documents\\GitHub\\mini-gpt\\ngram-V2.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_embedding(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram-V2.ipynb#X26sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1182\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1180\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1183\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1184\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x864 and 378x136)"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "import os, json\n",
    "\n",
    "os.makedirs(\"tmp/ngram\", exist_ok=True)\n",
    "\n",
    "with open(\"tmp/ngram/config.json\", \"w\") as f:\n",
    "    json.dump({\"chars\": chars, \"vocab_size\": vocab_size, \"n\": model.n}, f)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (torch.zeros(1, 10, dtype=torch.int32, device=device), True),\n",
    "    \"tmp/ngram/model.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch\", 1: \"time\"},\n",
    "        \"output\": {0: \"batch\", 1: \"time\"},\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
