{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-09-27 22:44:28--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'tmp/data.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  4% 3.80M 0s\n",
      "    50K .......... .......... .......... .......... ..........  9% 20.6M 0s\n",
      "   100K .......... .......... .......... .......... .......... 13% 7.33M 0s\n",
      "   150K .......... .......... .......... .......... .......... 18% 28.0M 0s\n",
      "   200K .......... .......... .......... .......... .......... 22% 23.9M 0s\n",
      "   250K .......... .......... .......... .......... .......... 27% 17.7M 0s\n",
      "   300K .......... .......... .......... .......... .......... 32% 23.8M 0s\n",
      "   350K .......... .......... .......... .......... .......... 36% 15.1M 0s\n",
      "   400K .......... .......... .......... .......... .......... 41% 18.7M 0s\n",
      "   450K .......... .......... .......... .......... .......... 45% 24.4M 0s\n",
      "   500K .......... .......... .......... .......... .......... 50% 20.3M 0s\n",
      "   550K .......... .......... .......... .......... .......... 55% 21.9M 0s\n",
      "   600K .......... .......... .......... .......... .......... 59% 17.1M 0s\n",
      "   650K .......... .......... .......... .......... .......... 64% 19.3M 0s\n",
      "   700K .......... .......... .......... .......... .......... 68% 21.7M 0s\n",
      "   750K .......... .......... .......... .......... .......... 73% 37.7M 0s\n",
      "   800K .......... .......... .......... .......... .......... 78% 22.2M 0s\n",
      "   850K .......... .......... .......... .......... .......... 82% 12.0M 0s\n",
      "   900K .......... .......... .......... .......... .......... 87% 13.2M 0s\n",
      "   950K .......... .......... .......... .......... .......... 91% 21.2M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 96% 36.6M 0s\n",
      "  1050K .......... .......... .......... .........            100%  179M=0.07s\n",
      "\n",
      "2023-09-27 22:44:28 (16.2 MB/s) - 'tmp/data.txt' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download file to tmp/data.txt\n",
    "!wget -O tmp/data.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('tmp/data.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print('text length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "chars = [\"[PAD]\", *sorted(list(set(text)))]\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 48, 48, 2, 59, 47, 44, 57, 44]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from character to index and vice versa\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([19, 48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44, 45,\n",
      "        54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40, 53, 64,\n",
      "         2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57,  2, 52, 44,  2,\n",
      "        58, 55, 44, 40, 50,  9,  1,  1, 14, 51, 51, 11,  1, 32, 55, 44, 40, 50,\n",
      "         7,  2, 58, 55, 44, 40, 50,  9,  1,  1, 19, 48, 57, 58, 59,  2, 16, 48,\n",
      "        59, 48, 65, 44, 53, 11,  1, 38, 54, 60], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# store in tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.int64, device=device)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # the 100 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44, 45,\n",
       "        54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40, 53, 64,\n",
       "         2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57,  2, 52, 44,  2,\n",
       "        58, 55, 44, 40, 50,  9,  1,  1, 14, 51, 51, 11,  1, 32, 55, 44, 40, 50,\n",
       "         7,  2, 58, 55, 44, 40, 50,  9,  1,  1, 19, 48, 57, 58, 59,  2, 16, 48,\n",
       "        59, 48, 65, 44, 53, 11,  1, 38, 54, 60,  2], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 100\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 10]) torch.int64 cuda:0\n",
      "targets:\n",
      "torch.Size([32, 10]) torch.int64 cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 10 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape, xb.dtype, xb.device)\n",
    "print('targets:')\n",
    "print(yb.shape, yb.dtype, yb.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n):\n",
    "        super().__init__()\n",
    "        super().to(device)\n",
    "        self.n = n\n",
    "        embed_size = vocab_size + 10 * n\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(n, embed_size))\n",
    "        self.adding_weight = nn.Parameter(torch.zeros(n))\n",
    "        self.fc = nn.Linear(embed_size, 200)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.final = nn.Linear(200, vocab_size)\n",
    "\n",
    "    # Create separate function for forward calculation\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 2, \"input shape should be (batch, time)\"\n",
    "        new_x = torch.zeros((x.shape[0], x.shape[1], self.n), dtype=torch.int64, device=device) - 69\n",
    "        # -69 is just a random number so if we see it we know something went wrong\n",
    "        for t in range(x.shape[1]):\n",
    "            for pos in range(self.n):\n",
    "                row = x[:, max(0, t - pos) : t + 1]\n",
    "                row = F.pad(row, (self.n - row.shape[1], 0), value=0)\n",
    "                row = F.pad(row, (self.n - row.shape[1], 0), value=0)\n",
    "                new_x[:, t] = row\n",
    "\n",
    "        x = torch.add(self.token_embedding(new_x), self.pos_embedding)\n",
    "        x = F.softmax(self.adding_weight, -1) @ x\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "    # def forward(self, x, targets=None):\n",
    "    #     if targets is None:\n",
    "    #         # we're doing inference, so we don't have targets\n",
    "    #         # if x is None, we're generating from scratch\n",
    "    #         if x is None:\n",
    "    #             x = batch_size\n",
    "\n",
    "    #         # generate x batches\n",
    "    #         if isinstance(x, int):\n",
    "    #             x = torch.zeros(x, 1, dtype=torch.int32, device=device)\n",
    "    #         return self.forward_calc(x), None\n",
    "\n",
    "    #     else:\n",
    "    #         # we're training, so we do have targets\n",
    "    #         logits = self.forward_calc(x)\n",
    "    #         B, T, C = logits.shape  # batch, time, channel\n",
    "    #         logits_flat = logits.view(B * T, C)\n",
    "    #         loss = F.cross_entropy(logits_flat, targets.view(B * T))\n",
    "\n",
    "    #         return logits_flat, loss\n",
    "\n",
    "    def loss(self, logits, targets):\n",
    "        B, T, C = logits.shape\n",
    "        logits_flat = logits.view(B * T, C)\n",
    "        loss = F.cross_entropy(logits_flat, targets.view(B * T))\n",
    "        return loss\n",
    "\n",
    "    def generate(self, x, max_len_new, temperature=1.0):\n",
    "        for _ in range(max_len_new):\n",
    "            logits = self(x)[:, -1] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "NGramLanguageModel                       [32, 10, 66]              959\n",
      "├─pos_embedding                                                    ├─952\n",
      "├─adding_weight                                                    └─7\n",
      "├─Embedding: 1-1                         [32, 10, 7, 136]          8,976\n",
      "│    └─weight                                                      └─8,976\n",
      "├─Linear: 1-2                            [32, 10, 200]             27,400\n",
      "│    └─weight                                                      ├─27,200\n",
      "│    └─bias                                                        └─200\n",
      "├─ReLU: 1-3                              [32, 10, 200]             --\n",
      "├─Linear: 1-4                            [32, 10, 66]              13,266\n",
      "│    └─weight                                                      ├─13,200\n",
      "│    └─bias                                                        └─66\n",
      "==========================================================================================\n",
      "Total params: 50,601\n",
      "Trainable params: 50,601\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 1.59\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.12\n",
      "Params size (MB): 0.20\n",
      "Estimated Total Size (MB): 3.32\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NGramLanguageModel(\n",
       "  (token_embedding): Embedding(66, 136, padding_idx=0)\n",
       "  (fc): Linear(in_features=136, out_features=200, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (final): Linear(in_features=200, out_features=66, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NGramLanguageModel(vocab_size, 7)\n",
    "summary(model, input_size=(32, 10), dtypes=[torch.long], verbose=2, device=device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGramLanguageModel(\n",
      "  (token_embedding): Embedding(66, 136, padding_idx=0)\n",
      "  (fc): Linear(in_features=136, out_features=200, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (final): Linear(in_features=200, out_features=66, bias=True)\n",
      ")\n",
      "logits: torch.Size([32, 10, 66])\n",
      "loss: tensor(4.1809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "rch o' the;peywaCjQY\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# logits, loss = model(xb, yb)\n",
    "logits = model(xb)\n",
    "loss = model.loss(logits, yb)\n",
    "print('logits:', logits.shape)\n",
    "print('loss:', loss)\n",
    "\n",
    "print(decode(model.generate(xb, 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.174\n",
      "step: 50, loss: 2.824\n",
      "step: 100, loss: 2.578\n",
      "step: 150, loss: 2.409\n",
      "step: 200, loss: 2.264\n",
      "step: 250, loss: 2.243\n",
      "step: 300, loss: 2.164\n",
      "step: 350, loss: 2.170\n",
      "step: 400, loss: 2.107\n",
      "step: 450, loss: 2.121\n",
      "step: 500, loss: 2.060\n",
      "step: 550, loss: 2.029\n",
      "step: 600, loss: 2.051\n",
      "step: 650, loss: 2.034\n",
      "step: 700, loss: 2.042\n",
      "step: 750, loss: 2.040\n",
      "step: 800, loss: 2.003\n",
      "step: 850, loss: 1.976\n",
      "step: 900, loss: 1.920\n",
      "step: 950, loss: 1.932\n",
      "step: 1000, loss: 1.984\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.004)\n",
    "batch_size = 256\n",
    "for step in range(1001):\n",
    "    xb, yb = get_batch('train')\n",
    "    # logits, loss = model(xb, yb)\n",
    "    logits = model(xb)\n",
    "    loss = model.loss(logits, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 50 == 0:\n",
    "        print(f'step: {step}, loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUCENTIONTERD IVER:\n",
      "Prove the beenuse o to arhank.\n",
      "\n",
      "AUS:\n",
      "Wheret in his the their fall the lost fring hor fot word and deis,\n",
      "And hear for willif and.\n",
      "\n",
      "ANGERITONEN LINA:\n",
      "We pertion.\n",
      "\n",
      "CKING EDWARD III:\n",
      "The see\n",
      "----\n",
      "LUCENTIO:\n",
      "'s this\n",
      "cI conde therence hat onor Penobles\n",
      "Your geand Carnise fear whose good mans and me Clarefoly band he's butir;\n",
      "And the thy im ble arsecusbe you, un comoners wiced not thou lcus he courgh;\n",
      "A\n",
      "----\n",
      "LUCENTES:\n",
      "Wher bones noord, and the vers of him you shot. Han thou we thou come; you she im'st, bace was if that with and us in heesatine, shat be corly worduen: me sould the the of unace\n",
      "det roves gin to h\n",
      "----\n",
      "LUCENTER:\n",
      "Andiedr him nirn bece:\n",
      "'Ttian ther! Gody geme know, hom onoter wifine the it and sort', and mer his londe?\n",
      "\n",
      "QUEENCUS:\n",
      "Verre as an whing on him then weatid: on gre be\n",
      "And to mobe!\n",
      "\n",
      "Leard live he di\n",
      "----\n",
      "LUCENTES:\n",
      "But our fle, 'eat his ding.\n",
      "\n",
      "KINGHARD IV:\n",
      "It I drown.\n",
      "Whith worcit: will odinfest thisce;\n",
      "As Resuming cout your he mor loved it the come of to stave of my penter:\n",
      "Nove and a giver my mothe vay say\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for x in model.generate(torch.tensor([encode(\"LUCENT\")] * 5, device=device), 200, 0.9):\n",
    "    print(decode(x.tolist()))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[47, 44, 51, 51, 54, 62,  2, 59, 47, 44,  2, 59, 47, 44,  2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(torch.tensor([encode(\"hello\")], device=device), 10, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0218, 0.0231, 0.0289, 0.0437, 0.1194, 0.2628, 0.5004],\n",
       "       device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(model.adding_weight, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import os, json\n",
    "\n",
    "os.makedirs(\"tmp/ngram\", exist_ok=True)\n",
    "\n",
    "with open(\"tmp/ngram/config.json\", \"w\") as f:\n",
    "    json.dump({\"chars\": chars, \"vocab_size\": vocab_size, \"n\": model.n}, f)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    torch.zeros(1, 10, dtype=torch.int32, device=device),\n",
    "    \"tmp/ngram/model.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch\", 1: \"time\"},\n",
    "        \"output\": {0: \"batch\", 1: \"time\"},\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
