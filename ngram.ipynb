{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-09-28 23:03:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'tmp/data.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  4% 2.78M 0s\n",
      "    50K .......... .......... .......... .......... ..........  9% 7.89M 0s\n",
      "   100K .......... .......... .......... .......... .......... 13% 12.5M 0s\n",
      "   150K .......... .......... .......... .......... .......... 18% 10.2M 0s\n",
      "   200K .......... .......... .......... .......... .......... 22% 16.8M 0s\n",
      "   250K .......... .......... .......... .......... .......... 27% 12.6M 0s\n",
      "   300K .......... .......... .......... .......... .......... 32% 19.3M 0s\n",
      "   350K .......... .......... .......... .......... .......... 36% 20.8M 0s\n",
      "   400K .......... .......... .......... .......... .......... 41% 19.1M 0s\n",
      "   450K .......... .......... .......... .......... .......... 45% 12.2M 0s\n",
      "   500K .......... .......... .......... .......... .......... 50% 19.6M 0s\n",
      "   550K .......... .......... .......... .......... .......... 55% 10.6M 0s\n",
      "   600K .......... .......... .......... .......... .......... 59% 20.1M 0s\n",
      "   650K .......... .......... .......... .......... .......... 64% 23.1M 0s\n",
      "   700K .......... .......... .......... .......... .......... 68% 8.70M 0s\n",
      "   750K .......... .......... .......... .......... .......... 73% 22.4M 0s\n",
      "   800K .......... .......... .......... .......... .......... 78% 67.6M 0s\n",
      "   850K .......... .......... .......... .......... .......... 82% 22.0M 0s\n",
      "   900K .......... .......... .......... .......... .......... 87% 5.31M 0s\n",
      "   950K .......... .......... .......... .......... .......... 91%  112M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 96% 8.15M 0s\n",
      "  1050K .......... .......... .......... .........            100% 17.0M=0.09s\n",
      "\n",
      "2023-09-28 23:03:50 (11.7 MB/s) - 'tmp/data.txt' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download file to tmp/data.txt\n",
    "!wget -O tmp/data.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('tmp/data.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print('text length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "chars = [\"[PAD]\", *sorted(list(set(text)))]\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 48, 48, 2, 59, 47, 44, 57, 44]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from character to index and vice versa\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([19, 48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44, 45,\n",
      "        54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40, 53, 64,\n",
      "         2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57,  2, 52, 44,  2,\n",
      "        58, 55, 44, 40, 50,  9,  1,  1, 14, 51, 51, 11,  1, 32, 55, 44, 40, 50,\n",
      "         7,  2, 58, 55, 44, 40, 50,  9,  1,  1, 19, 48, 57, 58, 59,  2, 16, 48,\n",
      "        59, 48, 65, 44, 53, 11,  1, 38, 54, 60], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# store in tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.int64, device=device)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # the 100 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44, 45,\n",
       "        54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40, 53, 64,\n",
       "         2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57,  2, 52, 44,  2,\n",
       "        58, 55, 44, 40, 50,  9,  1,  1, 14, 51, 51, 11,  1, 32, 55, 44, 40, 50,\n",
       "         7,  2, 58, 55, 44, 40, 50,  9,  1,  1, 19, 48, 57, 58, 59,  2, 16, 48,\n",
       "        59, 48, 65, 44, 53, 11,  1, 38, 54, 60,  2], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 100\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 10]) torch.int64 cuda:0\n",
      "targets:\n",
      "torch.Size([32, 10]) torch.int64 cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 10 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape, xb.dtype, xb.device)\n",
    "print('targets:')\n",
    "print(yb.shape, yb.dtype, yb.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n):\n",
    "        super().__init__()\n",
    "        super().to(device)\n",
    "        self.n = n\n",
    "        assert n >= 3, \"n should be at least 3\"\n",
    "        embedding_size = int(vocab_size / 1.5 + 10)\n",
    "        intermediate_size = vocab_size + 10 * n\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.fc = nn.Linear(embedding_size * n, intermediate_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.final = nn.Linear(intermediate_size, vocab_size)\n",
    "\n",
    "    # Create separate function for forward calculation\n",
    "    def forward(self, x, only_last=False):\n",
    "        assert len(x.shape) == 2, \"input shape should be (batch, time)\"\n",
    "\n",
    "        if only_last:\n",
    "            # pad time dim to at least n\n",
    "            x = x[:, -self.n :]\n",
    "            x = F.pad(x, (self.n - x.shape[1], 0), value=0)\n",
    "            B, N = x.shape\n",
    "            x = x.view(B, 1, N)\n",
    "        else:\n",
    "            new_x = torch.zeros((x.shape[0], x.shape[1], self.n), dtype=torch.int64, device=device) - 69\n",
    "            for time_index in range(x.shape[1]):\n",
    "                row = x[:, max(0, time_index - self.n + 1) : time_index + 1]\n",
    "                row = F.pad(row, (self.n - row.shape[1], 0), value=0)\n",
    "                new_x[:, time_index] = row\n",
    "            x = new_x\n",
    "\n",
    "        x = self.token_embedding(x)\n",
    "        x = x.view(x.shape[0], x.shape[1], -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, logits, targets):\n",
    "        B, T, C = logits.shape\n",
    "        logits_flat = logits.view(B * T, C)\n",
    "        loss = F.cross_entropy(logits_flat, targets.view(B * T))\n",
    "        return loss\n",
    "\n",
    "    def generate(self, x, max_len_new, temperature=1.0):\n",
    "        for _ in range(max_len_new):\n",
    "            logits = self(x, True)[:, -1] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Mult-Adds\n",
      "============================================================================================================================================\n",
      "NGramLanguageModel                       [256, 10]                 [256, 1, 66]              --                        --\n",
      "├─Embedding: 1-1                         [256, 1, 7]               [256, 1, 7, 54]           3,564                     912,384\n",
      "│    └─weight                                                                                └─3,564\n",
      "├─Linear: 1-2                            [256, 1, 378]             [256, 1, 136]             51,544                    13,195,264\n",
      "│    └─weight                                                                                ├─51,408\n",
      "│    └─bias                                                                                  └─136\n",
      "├─ReLU: 1-3                              [256, 1, 136]             [256, 1, 136]             --                        --\n",
      "├─Dropout: 1-4                           [256, 1, 136]             [256, 1, 136]             --                        --\n",
      "├─Linear: 1-5                            [256, 1, 136]             [256, 1, 66]              9,042                     2,314,752\n",
      "│    └─weight                                                                                ├─8,976\n",
      "│    └─bias                                                                                  └─66\n",
      "============================================================================================================================================\n",
      "Total params: 64,150\n",
      "Trainable params: 64,150\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 16.42\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 1.19\n",
      "Params size (MB): 0.26\n",
      "Estimated Total Size (MB): 1.46\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NGramLanguageModel(\n",
       "  (token_embedding): Embedding(66, 54, padding_idx=0)\n",
       "  (fc): Linear(in_features=378, out_features=136, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (final): Linear(in_features=136, out_features=66, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NGramLanguageModel(vocab_size, 7)\n",
    "summary(\n",
    "    model,\n",
    "    input_data=[torch.zeros((256, 10), dtype=torch.long, device=device), True],\n",
    "    verbose=2,\n",
    "    device=device,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGramLanguageModel(\n",
      "  (token_embedding): Embedding(66, 54, padding_idx=0)\n",
      "  (fc): Linear(in_features=378, out_features=136, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      "  (final): Linear(in_features=136, out_features=66, bias=True)\n",
      ")\n",
      "logits: torch.Size([256, 10, 66])\n",
      "loss: tensor(4.2405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "wful busindBJBihmh,$\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# logits, loss = model(xb, yb)\n",
    "logits = model(xb)\n",
    "loss = model.loss(logits, yb)\n",
    "print('logits:', logits.shape)\n",
    "print('loss:', loss)\n",
    "\n",
    "print(decode(model.generate(xb, 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.234\n",
      "step: 50, loss: 2.183\n",
      "step: 100, loss: 2.120\n",
      "step: 150, loss: 2.031\n",
      "step: 200, loss: 2.003\n",
      "step: 250, loss: 2.015\n",
      "step: 300, loss: 1.958\n",
      "step: 350, loss: 1.982\n",
      "step: 400, loss: 1.930\n",
      "step: 450, loss: 1.930\n",
      "step: 500, loss: 1.877\n",
      "step: 550, loss: 1.893\n",
      "step: 600, loss: 1.916\n",
      "step: 650, loss: 1.899\n",
      "step: 700, loss: 1.850\n",
      "step: 750, loss: 1.948\n",
      "step: 800, loss: 1.880\n",
      "step: 850, loss: 1.908\n",
      "step: 900, loss: 1.890\n",
      "step: 950, loss: 1.904\n",
      "step: 1000, loss: 1.967\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "batch_size = 256\n",
    "for step in range(1001):\n",
    "    xb, yb = get_batch('train')\n",
    "    # logits, loss = model(xb, yb)\n",
    "    logits = model(xb)\n",
    "    loss = model.loss(logits, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 50 == 0:\n",
    "        print(f'step: {step}, loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUCENTEO:\n",
      "Awd ther atd Juppet now death fore even make the god twar the now\n",
      "I gatill fawner love, and here great to minge to the stones tend been'd ineafes with heart; beth if seme in Corillonou, More his p\n",
      "----\n",
      "LUCENTIO:\n",
      "Abould: whe cood to grave spunist\n",
      "Wrot\n",
      "Tome this sted earder this much and\n",
      "Withese it\n",
      "fulling lords\n",
      "Is heory. servensiness spoked.\n",
      "\n",
      "yheress hen lory.\n",
      "\n",
      "LEONTES:\n",
      "Why, low lond arrus; my do the venry\n",
      "----\n",
      "LUCENTIO:\n",
      "How saus with the the prots\n",
      "Of Prone har fpray, ther, thome cot my Englory holdiert,\n",
      "Pould sweich as whene elcent nal tence\n",
      "Alouted.\n",
      "\n",
      "POLICINIUS:\n",
      "Withenhy,\n",
      "What insfor ermped he tuous,\n",
      "I lis evert\n",
      "----\n",
      "LUCENTIO:\n",
      "Nou dost kings carnou hus grovie noblone's.\n",
      "\n",
      "WARWICK:\n",
      "Oll quatthe condst in best\n",
      "Liking hather. Draw you, buy lital;\n",
      "When your pothie catitiod more, bast. But we,t look is ore naty. O copmucher, p\n",
      "----\n",
      "LUCENTIO:\n",
      "The heesty,\n",
      "Even that stal be not, and of a worn, not for Rer unns than stonour.\n",
      "\n",
      "GRUMO:\n",
      "Grest to enee, I thee, the \n",
      "Ono wer, more came spisent it driff bent me,\n",
      "Who, mirrot struch folld,\n",
      "Who thri\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for x in model.generate(torch.tensor([encode(\"LUCENT\")] * 5, device=device), 200, 0.9):\n",
    "    print(decode(x.tolist()))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[47, 44, 51, 51, 54, 62,  2, 59, 47, 44,  2, 59, 47, 44,  2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(torch.tensor([encode(\"hello\")], device=device), 10, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_9760\\2990080344.py:19: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if only_last:\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "import os, json\n",
    "\n",
    "os.makedirs(\"tmp/ngram\", exist_ok=True)\n",
    "\n",
    "with open(\"tmp/ngram/config.json\", \"w\") as f:\n",
    "    json.dump({\"chars\": chars, \"vocab_size\": vocab_size, \"n\": model.n}, f)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (torch.zeros(1, 1, dtype=torch.int32, device=device), True),\n",
    "    \"tmp/ngram/model.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch\", 1: \"time\"},\n",
    "        \"output\": {0: \"batch\", 1: \"time\"},\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
