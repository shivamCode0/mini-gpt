{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-09-25 22:28:16--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8001::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'tmp/data.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  4% 4.56M 0s\n",
      "    50K .......... .......... .......... .......... ..........  9% 7.48M 0s\n",
      "   100K .......... .......... .......... .......... .......... 13% 19.4M 0s\n",
      "   150K .......... .......... .......... .......... .......... 18% 8.90M 0s\n",
      "   200K .......... .......... .......... .......... .......... 22% 25.7M 0s\n",
      "   250K .......... .......... .......... .......... .......... 27% 12.2M 0s\n",
      "   300K .......... .......... .......... .......... .......... 32% 23.9M 0s\n",
      "   350K .......... .......... .......... .......... .......... 36% 14.2M 0s\n",
      "   400K .......... .......... .......... .......... .......... 41% 1008K 0s\n",
      "   450K .......... .......... .......... .......... .......... 45%  118M 0s\n",
      "   500K .......... .......... .......... .......... .......... 50%  199M 0s\n",
      "   550K .......... .......... .......... .......... .......... 55%  209M 0s\n",
      "   600K .......... .......... .......... .......... .......... 59%  168M 0s\n",
      "   650K .......... .......... .......... .......... .......... 64%  233M 0s\n",
      "   700K .......... .......... .......... .......... .......... 68%  135M 0s\n",
      "   750K .......... .......... .......... .......... .......... 73% 5.07M 0s\n",
      "   800K .......... .......... .......... .......... .......... 78%  149M 0s\n",
      "   850K .......... .......... .......... .......... .......... 82% 9.80M 0s\n",
      "   900K .......... .......... .......... .......... .......... 87%  158M 0s\n",
      "   950K .......... .......... .......... .......... .......... 91%  127M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 96%  155M 0s\n",
      "  1050K .......... .......... .......... .........            100%  228M=0.1s\n",
      "\n",
      "2023-09-25 22:28:16 (10.2 MB/s) - 'tmp/data.txt' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download file to tmp/data.txt\n",
    "!wget -O tmp/data.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('tmp/data.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print('text length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from character to index and vice versa\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# store in tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # the 100 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "# print('----')\n",
    "\n",
    "# for b in range(batch_size): # batch dimension\n",
    "#     for t in range(block_size): # time dimension\n",
    "#         context = xb[b, :t+1]\n",
    "#         target = yb[b,t]\n",
    "#         print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        if targets is None:\n",
    "            # we're doing inference, so we don't have targets\n",
    "            # in this case, x is a batch of contexts\n",
    "            # we need to return a batch of predictions\n",
    "\n",
    "            # if x is None, we're generating from scratch\n",
    "            if x is None:\n",
    "                x = batch_size\n",
    "\n",
    "            # generate x batches\n",
    "            if isinstance(x, int):\n",
    "                x = torch.zeros(x, 1, dtype=torch.long)\n",
    "            return self.table(x), None\n",
    "        else:\n",
    "            # we're training, so we do have targets\n",
    "            logits = self.table(x)\n",
    "            B, T, C = logits.shape  # batch, time, channel\n",
    "            logits_flat = logits.view(B * T, C)\n",
    "            loss = F.cross_entropy(logits_flat, targets.view(B * T))\n",
    "\n",
    "            return logits_flat, loss\n",
    "\n",
    "    def generate(self, x, max_len_new):\n",
    "        for _ in range(max_len_new):\n",
    "            logits, loss = self(x)\n",
    "            probs = F.softmax(logits[:, -1], dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLanguageModel(\n",
      "  (table): Embedding(65, 65)\n",
      ")\n",
      "logits: torch.Size([32, 65])\n",
      "loss: tensor(5.0364, grad_fn=<NllLossBackward>)\n",
      "Let's heXJamq!iui$\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "print(model)\n",
    "logits, loss = model(xb, yb)\n",
    "print('logits:', logits.shape)\n",
    "print('loss:', loss)\n",
    "\n",
    "print(decode(model.generate(xb, 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.532054424285889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 500, loss: 3.228471517562866\n",
      "step: 1000, loss: 2.726747751235962\n",
      "step: 1500, loss: 2.483638048171997\n",
      "step: 2000, loss: 2.423741340637207\n",
      "step: 2500, loss: 2.434169054031372\n",
      "step: 3000, loss: 2.4106667041778564\n",
      "step: 3500, loss: 2.3632164001464844\n",
      "step: 4000, loss: 2.420254945755005\n",
      "step: 4500, loss: 2.3026375770568848\n",
      "step: 5000, loss: 2.629218816757202\n",
      "step: 5500, loss: 2.5279862880706787\n",
      "step: 6000, loss: 2.4539856910705566\n",
      "step: 6500, loss: 2.515232563018799\n",
      "step: 7000, loss: 2.5164709091186523\n",
      "step: 7500, loss: 2.540008068084717\n",
      "step: 8000, loss: 2.460096836090088\n",
      "step: 8500, loss: 2.530355930328369\n",
      "step: 9000, loss: 2.407350540161133\n",
      "step: 9500, loss: 2.621864080429077\n",
      "step: 9999, loss: 2.3662900924682617\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.004)\n",
    "batch_size = 32\n",
    "for step in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 500 == 0:\n",
    "        print(f'step: {step}, loss: {loss.item()}')\n",
    "\n",
    "print(f'step: {step}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUCENTER: und howiste ty dyotrd,\n",
      "Theal lerno, y va f m my mulde ben s, r bet!\n",
      "AMAs sod ke alved.\n",
      "Thup sthe\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.tensor([encode(\"LUCENT\")]), 100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
