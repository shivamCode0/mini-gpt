{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-09-28 20:35:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8000::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'tmp/data.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  4% 3.40M 0s\n",
      "    50K .......... .......... .......... .......... ..........  9% 6.87M 0s\n",
      "   100K .......... .......... .......... .......... .......... 13% 16.0M 0s\n",
      "   150K .......... .......... .......... .......... .......... 18% 17.8M 0s\n",
      "   200K .......... .......... .......... .......... .......... 22% 10.2M 0s\n",
      "   250K .......... .......... .......... .......... .......... 27% 18.1M 0s\n",
      "   300K .......... .......... .......... .......... .......... 32% 16.0M 0s\n",
      "   350K .......... .......... .......... .......... .......... 36% 13.1M 0s\n",
      "   400K .......... .......... .......... .......... .......... 41% 19.1M 0s\n",
      "   450K .......... .......... .......... .......... .......... 45% 21.9M 0s\n",
      "   500K .......... .......... .......... .......... .......... 50% 21.8M 0s\n",
      "   550K .......... .......... .......... .......... .......... 55% 11.1M 0s\n",
      "   600K .......... .......... .......... .......... .......... 59% 12.4M 0s\n",
      "   650K .......... .......... .......... .......... .......... 64% 15.3M 0s\n",
      "   700K .......... .......... .......... .......... .......... 68% 13.5M 0s\n",
      "   750K .......... .......... .......... .......... .......... 73% 20.6M 0s\n",
      "   800K .......... .......... .......... .......... .......... 78% 20.7M 0s\n",
      "   850K .......... .......... .......... .......... .......... 82% 21.2M 0s\n",
      "   900K .......... .......... .......... .......... .......... 87% 19.0M 0s\n",
      "   950K .......... .......... .......... .......... .......... 91% 19.6M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 96% 11.1M 0s\n",
      "  1050K .......... .......... .......... .........            100% 16.6M=0.08s\n",
      "\n",
      "2023-09-28 20:35:05 (12.9 MB/s) - 'tmp/data.txt' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download file to tmp/data.txt\n",
    "!wget -O tmp/data.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 1115394\n"
     ]
    }
   ],
   "source": [
    "with open('tmp/data.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print('text length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "chars = [\"[PAD]\", *sorted(list(set(text)))]\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(\"vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 48, 48, 2, 59, 47, 44, 57, 44]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from character to index and vice versa\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([19, 48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44, 45,\n",
      "        54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40, 53, 64,\n",
      "         2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57,  2, 52, 44,  2,\n",
      "        58, 55, 44, 40, 50,  9,  1,  1, 14, 51, 51, 11,  1, 32, 55, 44, 40, 50,\n",
      "         7,  2, 58, 55, 44, 40, 50,  9,  1,  1, 19, 48, 57, 58, 59,  2, 16, 48,\n",
      "        59, 48, 65, 44, 53, 11,  1, 38, 54, 60], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# store in tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.int64, device=device)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # the 100 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 48, 57, 58, 59,  2, 16, 48, 59, 48, 65, 44, 53, 11,  1, 15, 44, 45,\n",
       "        54, 57, 44,  2, 62, 44,  2, 55, 57, 54, 42, 44, 44, 43,  2, 40, 53, 64,\n",
       "         2, 45, 60, 57, 59, 47, 44, 57,  7,  2, 47, 44, 40, 57,  2, 52, 44,  2,\n",
       "        58, 55, 44, 40, 50,  9,  1,  1, 14, 51, 51, 11,  1, 32, 55, 44, 40, 50,\n",
       "         7,  2, 58, 55, 44, 40, 50,  9,  1,  1, 19, 48, 57, 58, 59,  2, 16, 48,\n",
       "        59, 48, 65, 44, 53, 11,  1, 38, 54, 60,  2], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 100\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 10]) torch.int64 cuda:0\n",
      "targets:\n",
      "torch.Size([32, 10]) torch.int64 cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 10 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape, xb.dtype, xb.device)\n",
    "print('targets:')\n",
    "print(yb.shape, yb.dtype, yb.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n):\n",
    "        super().__init__()\n",
    "        super().to(device)\n",
    "        self.n = n\n",
    "        embed_size = vocab_size + 10 * n\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(n, embed_size))\n",
    "        self.adding_weight = nn.Parameter(torch.zeros(n))\n",
    "        self.fc = nn.Linear(embed_size, 200)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.final = nn.Linear(200, vocab_size)\n",
    "\n",
    "    # Create separate function for forward calculation\n",
    "    def forward(self, x, only_last=False):\n",
    "        assert len(x.shape) == 2, \"input shape should be (batch, time)\"\n",
    "\n",
    "        if only_last:\n",
    "            # pad time dim to at least n\n",
    "            x = F.pad(x, (self.n - 1, 0), value=0)\n",
    "            x = x.view(1, -1)\n",
    "        else:\n",
    "            new_x = torch.zeros((x.shape[0], x.shape[1], self.n), dtype=torch.int64, device=device) - 69\n",
    "            # -69 is just a random number so if we see it we know something went wrong\n",
    "            for t in range(x.shape[1]):\n",
    "                for pos in range(self.n):\n",
    "                    row = x[:, max(0, t - pos) : t + 1]\n",
    "                    row = F.pad(row, (self.n - row.shape[1], 0), value=0)\n",
    "                    row = F.pad(row, (self.n - row.shape[1], 0), value=0)\n",
    "                    new_x[:, t] = row\n",
    "            \n",
    "            x = new_x\n",
    "\n",
    "        x = torch.add(self.token_embedding(x), self.pos_embedding)\n",
    "        x = F.softmax(self.adding_weight, -1) @ x\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "    # def forward(self, x, targets=None):\n",
    "    #     if targets is None:\n",
    "    #         # we're doing inference, so we don't have targets\n",
    "    #         # if x is None, we're generating from scratch\n",
    "    #         if x is None:\n",
    "    #             x = batch_size\n",
    "\n",
    "    #         # generate x batches\n",
    "    #         if isinstance(x, int):\n",
    "    #             x = torch.zeros(x, 1, dtype=torch.int32, device=device)\n",
    "    #         return self.forward_calc(x), None\n",
    "\n",
    "    #     else:\n",
    "    #         # we're training, so we do have targets\n",
    "    #         logits = self.forward_calc(x)\n",
    "    #         B, T, C = logits.shape  # batch, time, channel\n",
    "    #         logits_flat = logits.view(B * T, C)\n",
    "    #         loss = F.cross_entropy(logits_flat, targets.view(B * T))\n",
    "\n",
    "    #         return logits_flat, loss\n",
    "\n",
    "    def loss(self, logits, targets):\n",
    "        B, T, C = logits.shape\n",
    "        logits_flat = logits.view(B * T, C)\n",
    "        loss = F.cross_entropy(logits_flat, targets.view(B * T))\n",
    "        return loss\n",
    "\n",
    "    def generate(self, x, max_len_new, temperature=1.0):\n",
    "        for _ in range(max_len_new):\n",
    "            logits = self(x)[:, -1] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat([x, next_token], dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "NGramLanguageModel                       [32, 10, 66]              959\n",
      "├─pos_embedding                                                    ├─952\n",
      "├─adding_weight                                                    └─7\n",
      "├─Embedding: 1-1                         [32, 10, 7, 136]          8,976\n",
      "│    └─weight                                                      └─8,976\n",
      "├─Linear: 1-2                            [32, 10, 200]             27,400\n",
      "│    └─weight                                                      ├─27,200\n",
      "│    └─bias                                                        └─200\n",
      "├─ReLU: 1-3                              [32, 10, 200]             --\n",
      "├─Linear: 1-4                            [32, 10, 66]              13,266\n",
      "│    └─weight                                                      ├─13,200\n",
      "│    └─bias                                                        └─66\n",
      "==========================================================================================\n",
      "Total params: 50,601\n",
      "Trainable params: 50,601\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 1.59\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.12\n",
      "Params size (MB): 0.20\n",
      "Estimated Total Size (MB): 3.32\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NGramLanguageModel(\n",
       "  (token_embedding): Embedding(66, 136, padding_idx=0)\n",
       "  (fc): Linear(in_features=136, out_features=200, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (final): Linear(in_features=200, out_features=66, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NGramLanguageModel(vocab_size, 7)\n",
    "summary(model, input_size=(32, 10), dtypes=[torch.long], verbose=2, device=device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGramLanguageModel(\n",
      "  (token_embedding): Embedding(66, 136, padding_idx=0)\n",
      "  (fc): Linear(in_features=136, out_features=200, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (final): Linear(in_features=200, out_features=66, bias=True)\n",
      ")\n",
      "logits: torch.Size([256, 10, 66])\n",
      "loss: tensor(4.2233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "e Lewis doE?T:Pt-B,g\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "# logits, loss = model(xb, yb)\n",
    "logits = model(xb)\n",
    "loss = model.loss(logits, yb)\n",
    "print('logits:', logits.shape)\n",
    "print('loss:', loss)\n",
    "\n",
    "print(decode(model.generate(xb, 10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 2.487\n",
      "step: 50, loss: 2.332\n",
      "step: 100, loss: 2.266\n",
      "step: 150, loss: 2.142\n",
      "step: 200, loss: 2.143\n",
      "step: 250, loss: 2.119\n",
      "step: 300, loss: 2.081\n",
      "step: 350, loss: 2.116\n",
      "step: 400, loss: 2.087\n",
      "step: 450, loss: 2.003\n",
      "step: 500, loss: 2.050\n",
      "step: 550, loss: 1.993\n",
      "step: 600, loss: 1.970\n",
      "step: 650, loss: 2.043\n",
      "step: 700, loss: 2.029\n",
      "step: 750, loss: 2.026\n",
      "step: 800, loss: 1.964\n",
      "step: 850, loss: 1.995\n",
      "step: 900, loss: 1.990\n",
      "step: 950, loss: 1.944\n",
      "step: 1000, loss: 2.008\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.004)\n",
    "batch_size = 256\n",
    "for step in range(1001):\n",
    "    xb, yb = get_batch('train')\n",
    "    # logits, loss = model(xb, yb)\n",
    "    logits = model(xb)\n",
    "    loss = model.loss(logits, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if step % 50 == 0:\n",
    "        print(f'step: {step}, loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUCENTIO:\n",
      "No the han 'd sulare injutay?\n",
      "\n",
      "CALRENNIUS:\n",
      "Ay; bectrothe as fettep and as abocsornown'd vis to theem.\n",
      "\n",
      "SICINIUS:\n",
      "And sols etherep of like chan but hee dirs with king lorks.\n",
      "\n",
      "DUCHARD VIII:\n",
      "For thui\n",
      "----\n",
      "LUCENTES:\n",
      "What bue thered is the wor and bee, lad simnirthis tretil the ea selay\n",
      "As\n",
      "Hand thattes!\n",
      "\n",
      "HENRY OF GE:\n",
      "Some ne'd recom that cike I amy word is mee withe oves oper to'st my condss. VOLANUMNIO:\n",
      "GHARD\n",
      "----\n",
      "LUCENTIO:\n",
      "Heren, gremad?\n",
      "\n",
      "KING EDWARD III:\n",
      "Watist, let ble hoper shathe neis prown to lecons.\n",
      "\n",
      "VIRGIUS:\n",
      "Ond you countir mored wethe of as of take;\n",
      "And thous to he seden your ymand byent and so thes.\n",
      "\n",
      "JISABE\n",
      "----\n",
      "LUCENTIO:\n",
      "Go was plable, for theen\n",
      "Ond of then lowe not bidese hadet agensen.\n",
      "\n",
      "KING Here's comell'd ie day! I\n",
      "Ne mewell the peed worlde yetin wash even ten we but yead to thes shattere ghis can sach thou no\n",
      "----\n",
      "LUCENTER:\n",
      "In cann igh on he mear, but usine,\n",
      "And to seod: an leteed of his paind unslet of your time to bes; the gran,\n",
      "No, the too sa isn to is go. ELIZABETH:\n",
      "For wand won.\n",
      "Or of the sithey more thy deake. \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for x in model.generate(torch.tensor([encode(\"LUCENT\")] * 5, device=device), 200, 0.9):\n",
    "    print(decode(x.tolist()))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[47, 44, 51, 51, 54, 62,  2, 59, 47, 44,  2, 59, 47, 44,  2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(torch.tensor([encode(\"hello\")], device=device), 10, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0218, 0.0231, 0.0289, 0.0437, 0.1194, 0.2628, 0.5004],\n",
       "       device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(model.adding_weight, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_21120\\2721351841.py:18: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if only_last:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (7) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shiva\\Documents\\GitHub\\mini-gpt\\ngram.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtmp/ngram/config.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     json\u001b[39m.\u001b[39mdump({\u001b[39m\"\u001b[39m\u001b[39mchars\u001b[39m\u001b[39m\"\u001b[39m: chars, \u001b[39m\"\u001b[39m\u001b[39mvocab_size\u001b[39m\u001b[39m\"\u001b[39m: vocab_size, \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m: model\u001b[39m.\u001b[39mn}, f)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     (torch\u001b[39m.\u001b[39;49mzeros(\u001b[39m1\u001b[39;49m, \u001b[39m10\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint32, device\u001b[39m=\u001b[39;49mdevice), \u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtmp/ngram/model.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     input_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     output_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49m{\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\u001b[39m0\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m: {\u001b[39m0\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     },\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\onnx\\utils.py:504\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[0;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[0;32m    188\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    204\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    205\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \n\u001b[0;32m    208\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 504\u001b[0m     _export(\n\u001b[0;32m    505\u001b[0m         model,\n\u001b[0;32m    506\u001b[0m         args,\n\u001b[0;32m    507\u001b[0m         f,\n\u001b[0;32m    508\u001b[0m         export_params,\n\u001b[0;32m    509\u001b[0m         verbose,\n\u001b[0;32m    510\u001b[0m         training,\n\u001b[0;32m    511\u001b[0m         input_names,\n\u001b[0;32m    512\u001b[0m         output_names,\n\u001b[0;32m    513\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[0;32m    514\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[0;32m    515\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[0;32m    516\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[0;32m    517\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[0;32m    518\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[0;32m    519\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[0;32m    520\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\onnx\\utils.py:1529\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[0;32m   1526\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1527\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[1;32m-> 1529\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[0;32m   1530\u001b[0m     model,\n\u001b[0;32m   1531\u001b[0m     args,\n\u001b[0;32m   1532\u001b[0m     verbose,\n\u001b[0;32m   1533\u001b[0m     input_names,\n\u001b[0;32m   1534\u001b[0m     output_names,\n\u001b[0;32m   1535\u001b[0m     operator_export_type,\n\u001b[0;32m   1536\u001b[0m     val_do_constant_folding,\n\u001b[0;32m   1537\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[0;32m   1538\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m   1539\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[0;32m   1540\u001b[0m )\n\u001b[0;32m   1542\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[0;32m   1544\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[0;32m   1545\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\onnx\\utils.py:1111\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[1;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m   1110\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[1;32m-> 1111\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[0;32m   1112\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[0;32m   1114\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\onnx\\utils.py:987\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m    982\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[0;32m    983\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     )\n\u001b[0;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 987\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[0;32m    988\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m    989\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\onnx\\utils.py:891\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m    889\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[0;32m    890\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 891\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[0;32m    892\u001b[0m     model,\n\u001b[0;32m    893\u001b[0m     args,\n\u001b[0;32m    894\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    895\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    896\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    897\u001b[0m )\n\u001b[0;32m    898\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[0;32m    900\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\jit\\_trace.py:1184\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[1;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[0;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m   1183\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m-> 1184\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1185\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\jit\\_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[1;32m--> 127\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[0;32m    128\u001b[0m     wrapper,\n\u001b[0;32m    129\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[0;32m    130\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[0;32m    131\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[0;32m    133\u001b[0m )\n\u001b[0;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[0;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\jit\\_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    117\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[1;32m--> 118\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    120\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\shiva\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\nn\\modules\\module.py:1182\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1180\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1183\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1184\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "\u001b[1;32mc:\\Users\\shiva\\Documents\\GitHub\\mini-gpt\\ngram.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m             new_x[:, t] \u001b[39m=\u001b[39m row\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     x \u001b[39m=\u001b[39m new_x\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49madd(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken_embedding(x), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_embedding)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39madding_weight, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m@\u001b[39m x\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shiva/Documents/GitHub/mini-gpt/ngram.ipynb#X26sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (7) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "import os, json\n",
    "\n",
    "os.makedirs(\"tmp/ngram\", exist_ok=True)\n",
    "\n",
    "with open(\"tmp/ngram/config.json\", \"w\") as f:\n",
    "    json.dump({\"chars\": chars, \"vocab_size\": vocab_size, \"n\": model.n}, f)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (torch.zeros(1, 10, dtype=torch.int32, device=device), True),\n",
    "    \"tmp/ngram/model.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch\", 1: \"time\"},\n",
    "        \"output\": {0: \"batch\", 1: \"time\"},\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
